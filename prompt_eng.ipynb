{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering provent techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Notes (CoN)\n",
    "\n",
    "#### IMPLEMENTATION DETAILS\n",
    "In the retrieval phase, we employed DPR Karpukhin et al. (2020) to retrieve documents from\n",
    "Wikipedia. We accessed the model via direct loading from the official DPR repository hosted on\n",
    "GitHub. Subsequent to retrieval, our fine-tuning process for the LLaMA-2 Touvron et al. (2023)\n",
    "model runs for 3 epochs with a batch size set to 128, leveraging the DeepSpeed library Rasley et al.\n",
    "(2020) and the ZeRO optimizer Ma et al. (2021), with bfloat16 precision. The learning rates are set to\n",
    "{1e-6, 2e-6, 5e-6, 1e-5, 2e-5}, and the empirical results indicated that 5e-6 yielded the best model\n",
    "performance, hence we standardized the learning rate for all reported numbers. Greedy decoding is\n",
    "applied during inference on all experiments to ensure deterministic generations.\n",
    "\n",
    "#### INSTRUCTION PROMPTS\n",
    "(1) For standard RALM, the instruction is:\n",
    "Task Description: The primary objective is to briefly answer a specific question.\n",
    "(1) For RALM with CON, the instruction is:\n",
    "Task Description:\n",
    "1. Read the given question and five Wikipedia passages to gather relevant information.\n",
    "2. Write reading notes summarizing the key points from these passages.\n",
    "3. Discuss the relevance of the given question and Wikipedia passages.\n",
    "4. If some passages are relevant to the given question, provide a brief answer based on the passages.\n",
    "5. If no passage is relevant, direcly provide answer without considering the passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
